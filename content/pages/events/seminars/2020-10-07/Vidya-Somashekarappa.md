---
date: 2020-10-07
title: "What's your occipital lobe looking at? Gaze patterns and non-verbal cue detection"
lecturer: "Vidya Somashekarappa"
duration: "2 hours"
venue: "Gothenburg"
slides: 
---



 In this talk, I will discuss the topic of gaze prediction and my annotation effort to produce a data source that can help with automated dialogue systems. Specifically, dialogue systems need a way to distinguish between social and referential gaze in order to react more naturally to the cues given by human users. I will present an annotation scheme as well as an analysis of collected data for dyadic conversations that reveal gaze behaviour patterns that are significant to the question of the role of gaze in interaction.

Temporal patterns of non-verbal behaviour cue detection and interpretation in a given scenario are natural for humans and mostly an unconscious process, but identifying them is difficult for a robot or avatar. Research has shown that from birth on, humans prefer to look at faces that engage in reciprocated gaze, and that healthy babies show enhanced neural processing of direct eye gaze. Eye movements have been shown in many domains to have significant effects on decision-making. Machines that can sense and respond to these in a meaningful way are welcomed by the users because of the efficient interaction and bonding experience. Unsurprisingly, eye contact is closely linked to our emotions and consequently affects our behaviors.

The ultimate goal of this research project is to understand higher level behaviours, such as attention and eye gaze during conversation and how these findings are synthesized and can be utilized in the domains of Human–Robot Interaction and Human–Computer Interaction.


